
\subsection{Comparison between Inspection and User Testing}
The two approaches, inspection and user testing, are distinctly different and can yield divergent results. Inspection involves analysing the design without directly involving users, its main objective is to identify potential issues in the design, such as violations of usability heuristics, programming errors, or visual inconsistencies. However, inspection has a significant limitation: it does not provide insights into users' actual experiences, hence, user testing is employed, which involves real users performing specific tasks on the product. During user testing, users interact with the website or application under natural conditions, while researchers observe and gather data on their actions, feedback, and difficulties. 
\newline In practice, inspection detects design issues recognized by experts, while user testing reveals actual and specific problems encountered by users during product usage.
\newline

During our inspection of the UNICEF website, we identified several design issues, highlighting recurring problems across various pages. There is a noticeable lack of coherence in the content, with redundant information and suboptimal organization, meanwhile Navigation presents challenges related to a vague hierarchy, the absence of breadcrumbs, and users' difficulty in orienting themselves on the site. Additionally, the website's presentation shows visual inconsistencies, potentially eliciting feelings of confusion and disorientation among users. More specifically, the average scores obtained for the analysed categories were 2.5 out of 5 for content, 2.92 out of 5 for navigation, and 3.06 out of 5 for presentation. From this, we deduced that, although the site is usable for accessing information, issues related to hierarchy, coherence, and information overload pose a significant risk to user interaction.
\newline

After the inspection phase, we proceeded with user testing involving 20 participants, and defining tasks based on the main issues identified. All tasks were completed with a high success rate by users, with average completion times consistent with our expectations, although in some cases they exceeded them. The evaluated categories include Usability, Content, Interaction and Navigation, Aesthetics, Organization, and Consistency. Overall, more criticisms than praises were identified for each category, with particular attention to criticisms in the Interaction and Navigation, and Organization categories. Despite the high completion rate, users reported significant frustration and a high cognitive load during navigation.
\newline

Comparing the two methodologies directly, it emerges that the majority of issues identified during the inspection were confirmed during user testing, encompassing navigation and content problems, and allowing for the identification of new concerns, particularly regarding organization, this contributed to a deeper understanding of user experiences. Thus, while the inspection provided an initial analysis of the UNICEF website's design issues, user testing both confirmed many of these problems and uncovered new ones, offering a more comprehensive perspective on user experiences and areas for improvement. Both methods proved complementary in providing a holistic view of the website's usability challenges.
\newline

The slight discrepancies observed probably have their origins in the differences in our backgrounds as authors of the inspection and those of the participants in the user testing. We, in fact, primarily come from technical backgrounds focused on programming and design, whereas most testers have diverse backgrounds, noticing different details and overlooking others, such as aesthetic aspects. Additionally, it's important to consider that during the inspection, we carefully scrutinized the site multiple times, while testers had their first interaction with the site, completing relatively quick tasks.
